import streamlit as st
from PIL import Image
import requests
from io import BytesIO

# Configuration de la page 
st.set_page_config(
    page_title="Analyse e-commerce",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Injection du script de gestion d'erreurs
st.components.v1.html("""
<script>
window.onerror = function(message, source, lineno, colno, error) {
  if (message.includes('removeChild') || 
      message.includes('NotFoundError') ||
      message.includes('n≈ìud √† supprimer')) {
    console.warn("[Streamlit] Erreur DOM ignor√©e:", message);
    return true;
  }
  return false;
};
</script>
""", height=0)

# Import des biblioth√®ques
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import mlxtend
import squarify      
from mlxtend.frequent_patterns import fpgrowth, association_rules
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.decomposition import PCA
from sklearn import metrics
import warnings

# Ignorer les warnings
warnings.filterwarnings("ignore")

# Fonctions utilitaires
def load_data(uploaded_file):
    """Charge les donn√©es depuis un fichier upload√©"""
    try:
        if uploaded_file.name.endswith('.csv'):
            # Essayer diff√©rents encodages
            try:
                df = pd.read_csv(uploaded_file, encoding="ISO-8859-1")
            except:
                df = pd.read_csv(uploaded_file, encoding="utf-8")
        elif uploaded_file.name.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(uploaded_file)
        else:
            st.error("Format de fichier non support√©. Veuillez uploader un fichier CSV ou Excel.")
            return None
        
        st.success("Donn√©es charg√©es avec succ√®s!")
        return df
    except Exception as e:
        st.error(f"Erreur lors du chargement du fichier: {e}")
        return None

def clean_data(df):
    """Nettoie les donn√©es"""
    # Suppression des lignes enti√®rement vides
    df = df.dropna(how='all')
    
    # Suppression des doublons
    df = df.drop_duplicates()
    
    # Suppression des transactions sans CustomerID
    if 'CustomerID' in df.columns:
        df = df[~df['CustomerID'].isna()]
    
    # Suppression des valeurs manquantes restantes
    df = df.dropna()
    
    # SOLUTION DEFINITIVE POUR LES DATES
    if 'InvoiceDate' in df.columns:
        # Convertir en string puis en datetime
        df['InvoiceDate'] = df['InvoiceDate'].astype(str)
        
        # Correction 1: Essayer plusieurs formats de date
        try:
            df['InvoiceDate'] = pd.to_datetime(
                df['InvoiceDate'], 
                errors='coerce',
                infer_datetime_format=True
            )
        except:
            # Si √©chec, essayer manuellement les formats communs
            for fmt in ['%Y-%m-%d %H:%M:%S', '%d/%m/%Y %H:%M', '%m/%d/%Y %H:%M']:
                try:
                    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format=fmt, errors='coerce')
                    break
                except:
                    continue
        
        # Supprimer les dates invalides
        df = df.dropna(subset=['InvoiceDate'])
    
    # Correction 2: Convertir les colonnes num√©riques probl√©matiques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # Convertir en float pour √©viter les probl√®mes de type
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Ajout du montant total si possible
    if 'Quantity' in df.columns and 'UnitPrice' in df.columns:
        df['Montant'] = df['Quantity'] * df['UnitPrice']
    
    # Correction 3: Supprimer les valeurs infinies
    df = df.replace([np.inf, -np.inf], np.nan).dropna()
    
    # R√©initialiser les index
    df.reset_index(drop=True, inplace=True)
    
    return df

def global_stats(df):
    """Calcule les statistiques globales du DataFrame"""
    stats = {
        "Indicateur": [
            "Nombre de variables",
            "Nombre d'observations",
            "Nombre de valeurs manquantes",
            "Pourcentage de valeurs manquantes",
            "Nombre de lignes dupliqu√©es",
            "Pourcentage de lignes dupliqu√©es",
            "Nombre de lignes enti√®rement vides",
            "Pourcentage de lignes vides",
            "Nombre de colonnes vides",
            "Pourcentage de colonnes vides"
        ],
        "Valeur": [
            df.shape[1],
            df.shape[0],
            df.isna().sum().sum(),
            "{:.2%}".format(df.isna().sum().sum() / df.size),
            df.duplicated().sum(),
            "{:.2%}".format(df.duplicated().sum() / len(df)),
            df.isna().all(axis=1).sum(),
            "{:.2%}".format(df.isna().all(axis=1).sum() / len(df)),
            df.isnull().all().sum(),
            "{:.2%}".format(df.isnull().all().sum() / df.shape[1])
        ]
    }

    return pd.DataFrame(stats)

def safe_display_dataframe(df, max_rows=10000):
    """Affiche un DataFrame de mani√®re s√©curis√©e"""
    if len(df) > max_rows:
        st.warning(f"Le DataFrame contient trop de lignes ({len(df)}). Affichage limit√© √† {max_rows} lignes.")
        df = df.head(max_rows)
    
    # Conversion des types complexes en string
    for col in df.columns:
        if df[col].dtype == object:
            try:
                # Essayer de convertir les objets complexes en string
                df[col] = df[col].astype(str)
            except:
                # Si √©chec, supprimer la colonne
                st.warning(f"Colonne '{col}' supprim√©e car type non support√©: {df[col].dtype}")
                df = df.drop(col, axis=1)
    
    st.dataframe(df)

def show_descriptive_stats(df):
    """Affiche des statistiques descriptives compl√®tes avec visualisations"""
    st.header("üìä Analyse Descriptive Compl√®te")
    
    tab_stats, tab_dist, tab_box, tab_bivar = st.tabs([
        "Statistiques de Base", 
        "Distributions", 
        "Bo√Ætes √† Moustaches", 
        "Analyse Bivari√©e"
    ])
    
    with tab_stats:
        st.subheader("Statistiques Summaries")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Aper√ßu des Donn√©es**")
            safe_display_dataframe(df.head())
            
            st.write("**Types des Variables**")
            types_df = pd.DataFrame(df.dtypes, columns=['Type']).reset_index()
            types_df.columns = ['Variable', 'Type']
            safe_display_dataframe(types_df)
            
        with col2:
            st.write("**Statistiques Num√©riques**")
            # Filtrer uniquement les colonnes num√©riques et exclure CustomerID
            num_cols = df.select_dtypes(include=np.number).columns
            
            # CORRECTION: Exclure CustomerID des statistiques num√©riques
            if 'CustomerID' in num_cols:
                num_cols = num_cols.drop('CustomerID')
            
            if not num_cols.empty:
                safe_display_dataframe(df[num_cols].describe())
            else:
                st.warning("Aucune colonne num√©rique trouv√©e")
            
            st.write("**Valeurs Manquantes**")
            missing = df.isnull().sum().reset_index()
            missing.columns = ['Variable', 'Nombre']
            missing['Pourcentage'] = (missing['Nombre'] / len(df)) * 100
            safe_display_dataframe(missing)
    
    with tab_dist:
        st.subheader("Distributions des Variables")
        
        num_cols = df.select_dtypes(include=np.number).columns.tolist()
        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
        col1, col2 = st.columns(2)
        
        with col1:
            if num_cols:
                selected_num = st.selectbox("Variable num√©rique", num_cols, key="num_var")
                
                if selected_num:
                    # V√©rifier qu'il y a des valeurs √† afficher
                    if df[selected_num].notna().sum() > 0:
                        fig = px.histogram(
                            df, 
                            x=selected_num, 
                            nbins=50, 
                            title=f"Distribution de {selected_num}"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.warning(f"Aucune donn√©e valide pour la colonne {selected_num}")
            else:
                st.warning("Aucune variable num√©rique trouv√©e")
        
        with col2:
            if cat_cols:
                selected_cat = st.selectbox("Variable cat√©gorielle", cat_cols, key="cat_var")
                
                if selected_cat:
                    # Limiter √† un nombre raisonnable de cat√©gories
                    unique_count = df[selected_cat].nunique()
                    if unique_count > 50:
                        st.warning(f"Trop de cat√©gories ({unique_count}). Affichage limit√© aux 20 premi√®res.")
                        top_cats = df[selected_cat].value_counts().head(20).index
                        count_data = df[df[selected_cat].isin(top_cats)][selected_cat].value_counts().reset_index()
                    else:
                        count_data = df[selected_cat].value_counts().reset_index()
                    
                    count_data.columns = ['Cat√©gorie', 'Count']
                    
                    if not count_data.empty:
                        fig = px.bar(
                            count_data, 
                            x='Cat√©gorie', 
                            y='Count',
                            title=f"Distribution de {selected_cat}"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.warning("Aucune donn√©e √† afficher")
            else:
                st.warning("Aucune variable cat√©gorielle trouv√©e")
    
    with tab_box:
        st.subheader("Bo√Ætes √† Moustaches et Variance")
        
        num_cols = df.select_dtypes(include=np.number).columns.tolist()
        if num_cols:
            selected_box = st.multiselect(
                "Variables num√©riques √† comparer", 
                num_cols, 
                default=num_cols[:min(3, len(num_cols))],
                key="box_vars"
            )
            
            if selected_box:
                # Filtrer les colonnes avec des donn√©es valides
                valid_cols = [col for col in selected_box if df[col].notna().sum() > 0]
                
                if valid_cols:
                    # Cr√©er un DataFrame avec uniquement les colonnes valides
                    plot_df = df[valid_cols].melt(value_vars=valid_cols).dropna()
                    
                    if not plot_df.empty:
                        fig = px.box(
                            plot_df,
                            x='variable',
                            y='value',
                            title="Comparaison des Distributions"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        st.subheader("Matrice de Corr√©lation")
                        corr_matrix = df[valid_cols].corr().round(2)
                        
                        fig = px.imshow(
                            corr_matrix,
                            text_auto=True,
                            aspect="auto",
                            title="Corr√©lations entre Variables"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.warning("Aucune donn√©e valide pour l'affichage")
                else:
                    st.warning("Aucune colonne valide s√©lectionn√©e")
        else:
            st.warning("Aucune variable num√©rique trouv√©e pour les bo√Ætes √† moustaches")
    
    with tab_bivar:
        st.subheader("Analyse Bivari√©e")
        
        num_cols = df.select_dtypes(include=np.number).columns.tolist()
        if len(num_cols) >= 2:
            col1, col2 = st.columns(2)
            
            with col1:
                x_var = st.selectbox("Variable X", num_cols, key="x_var")
            with col2:
                y_var = st.selectbox("Variable Y", num_cols, index=1, key="y_var")
            
            # V√©rifier que les colonnes ont des donn√©es
            if df[[x_var, y_var]].notna().any().all():
                fig = px.scatter(
                    df,
                    x=x_var,
                    y=y_var,
                    title=f"Relation entre {x_var} et {y_var}"
                )
                st.plotly_chart(fig, use_container_width=True)
                
                if num_cols:
                    corr = df[[x_var, y_var]].corr().iloc[0,1]
                    st.metric("Coefficient de Corr√©lation", f"{corr:.2f}")
            else:
                st.warning("Donn√©es insuffisantes pour g√©n√©rer le graphique")
        else:
            st.warning("Au moins 2 variables num√©riques requises pour l'analyse bivari√©e")

def perform_fpgrowth_analysis(df):
    """Effectue l'analyse FP-Growth"""
    # V√©rification des colonnes n√©cessaires
    required_cols = ['InvoiceNo', 'Description', 'Quantity']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        st.error(f"Colonnes manquantes pour FP-Growth: {', '.join(missing_cols)}")
        return
    
    with st.spinner("Pr√©paration des donn√©es pour FP-Growth..."):
        try:
            # Cr√©ation du panier
            basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].count().unstack().fillna(0)
            basket = basket.applymap(lambda x: 1 if x > 0 else 0)
            
            # Supprimer les colonnes avec des noms manquants
            basket = basket.loc[:, ~basket.columns.isin([np.nan, None, ''])]
            
            # Limiter aux colonnes les plus fr√©quentes si trop nombreuses
            if len(basket.columns) > 10:
                st.warning("Trop de produits ({}). Utilisation des 10 plus fr√©quents.".format(len(basket.columns)))
                top_products = basket.sum().sort_values(ascending=False).head(100).index
                basket = basket[top_products]
        except Exception as e:
            st.error(f"Erreur lors de la pr√©paration des donn√©es: {e}")
            return
    
    col1, col2 = st.columns(2)
    with col1:
        min_support = st.slider("Support minimum", 0.01, 0.5, 0.01, 0.01, key="min_support")
    with col2:
        min_lift = st.slider("Lift minimum", 1.0, 10.0, 1.0, 0.1, key="min_lift")
    
    if st.button("Ex√©cuter FP-Growth"):
        with st.spinner("Calcul des r√®gles d'association..."):
            try:
                frequent_itemsets = fpgrowth(basket, min_support=min_support, use_colnames=True)
                
                if frequent_itemsets.empty:
                    st.warning("Aucun itemset fr√©quent trouv√© avec ce support minimum")
                    return
                
                rules = association_rules(frequent_itemsets, metric='lift', min_threshold=min_lift)
                
                if rules.empty:
                    st.warning("Aucune r√®gle d'association trouv√©e avec ces param√®tres")
                    return
                
                # Nettoyage des r√®gles
                rules['pair_key'] = rules.apply(lambda row: tuple(sorted([row['antecedents'], row['consequents']])), axis=1)
                rules = rules.drop_duplicates(subset='pair_key')
                rules.drop(columns='pair_key', inplace=True)
                
                # Conversion en liste
                rules['antecedents'] = rules['antecedents'].apply(lambda x: list(x))
                rules['consequents'] = rules['consequents'].apply(lambda x: list(x))
                
                # Formatage pour l'affichage
                rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ", ".join(x))
                rules['consequents_str'] = rules['consequents'].apply(lambda x: ", ".join(x))
            except Exception as e:
                st.error(f"Erreur lors du calcul des r√®gles: {e}")
                return
        
        st.success(f"FP-Growth termin√©! {len(rules)} r√®gles g√©n√©r√©es")
        
        # Affichage des r√©sultats
        st.subheader("Top 20 des R√®gles d'Association")
        safe_display_dataframe(rules.sort_values(by='lift', ascending=False).head(20))
        
        # Visualisation
        st.subheader("Visualisation des R√®gles")
        
        col1, col2 = st.columns(2)
        with col1:
            top_n = st.slider("Nombre de r√®gles √† visualiser", 5, 50, 10, key="top_n_fp")
            
            fig1 = px.scatter(
                rules.head(top_n),
                x='support',
                y='confidence',
                size='lift',
                color='lift',
                hover_data=['antecedents_str', 'consequents_str'],
                title="Support vs Confiance"
            )
            st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            fig2 = px.bar(
                rules.sort_values(by='lift', ascending=False).head(top_n),
                x='lift',
                y='antecedents_str',
                color='consequents_str',
                orientation='h',
                title="Top R√®gles par Lift"
            )
            st.plotly_chart(fig2, use_container_width=True)
        
        # T√©l√©chargement des r√©sultats
        st.subheader("T√©l√©charger les r√©sultats")
        csv = rules.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="T√©l√©charger les r√®gles en CSV",
            data=csv,
            file_name="regles_association.csv",
            mime='text/csv'
        )

def perform_kmeans_analysis(df):
    """Effectue l'analyse K-means avec contrat de maintenance"""
    st.subheader("S√©lection des Variables pour le Clustering")
    
    # Pr√©paration des donn√©es RFM
    if 'InvoiceDate' in df.columns and 'InvoiceNo' in df.columns and 'Montant' in df.columns:
        # Convertir InvoiceDate en datetime si n√©cessaire
        if not pd.api.types.is_datetime64_any_dtype(df['InvoiceDate']):
            try:
                df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
            except:
                st.error("Impossible de convertir les dates")
                return
            
        date_ref = df['InvoiceDate'].max() + timedelta(days=1)
        rfm = df.groupby('CustomerID').agg({
            'InvoiceDate': lambda x: (date_ref - x.max()).days,
            'InvoiceNo': 'nunique',
            'Montant': 'sum'
        }).reset_index()
        rfm.columns = ['CustomerID', 'Recence', 'Frequence', 'Montant']
        df_rfm = rfm
    else:
        st.warning("Colonnes manquantes pour le calcul RFM. Utilisation de toutes les colonnes num√©riques.")
        df_rfm = df.select_dtypes(include=np.number)
    
    # S√©lection des variables
    num_cols = df_rfm.select_dtypes(include=np.number).columns.tolist()
    selected_cols = st.multiselect(
        "S√©lectionnez les variables pour le clustering", 
        num_cols, 
        default=num_cols[:min(3, len(num_cols))],
        key="kmeans_vars"
    )
    
    if not selected_cols:
        st.warning("Veuillez s√©lectionner au moins une variable num√©rique.")
        return
    
    # Standardisation des donn√©es
    scaler = StandardScaler()
    
    # Supprimer les valeurs manquantes et infinies
    df_rfm_clean = df_rfm[selected_cols].replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(df_rfm_clean) == 0:
        st.error("Aucune donn√©e valide apr√®s nettoyage")
        return
        
    scaled_data = scaler.fit_transform(df_rfm_clean)
    
    # D√©termination du nombre de clusters
    st.subheader("D√©termination du Nombre Optimal de Clusters")
    max_clusters = st.slider("Nombre maximum de clusters √† tester", 3, 15, 10, key="max_clusters")
    
    if st.button("Trouver le nombre optimal de clusters"):
        with st.spinner("Calcul en cours..."):
            wcss = []  # Within-Cluster Sum of Square
            silhouette_scores = []
            
            for i in range(2, max_clusters + 1):
                try:
                    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
                    kmeans.fit(scaled_data)
                    wcss.append(kmeans.inertia_)
                    
                    if i > 1:  # Silhouette n√©cessite au moins 2 clusters
                        silhouette_scores.append(silhouette_score(scaled_data, kmeans.labels_))
                except Exception as e:
                    st.error(f"Erreur avec {i} clusters: {e}")
                    break
            
            # Trouver le meilleur nombre de clusters bas√© sur le score de silhouette
            if silhouette_scores:
                best_n = np.argmax(silhouette_scores) + 2
            else:
                best_n = 3
                st.warning("Impossible de calculer le score de silhouette, utilisation par d√©faut de 3 clusters")
        
        # Affichage des r√©sultats
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("M√©thode du Coude")
            fig1, ax1 = plt.subplots()
            ax1.plot(range(2, max_clusters + 1), wcss, marker='o')
            ax1.set_title('M√©thode du Coude')
            ax1.set_xlabel('Nombre de clusters')
            ax1.set_ylabel('WCSS')
            st.pyplot(fig1)
        
        with col2:
            st.subheader("Score de Silhouette")
            fig2, ax2 = plt.subplots()
            ax2.plot(range(2, len(silhouette_scores)+2), silhouette_scores, marker='o', color='green')
            ax2.set_title('Score de Silhouette')
            ax2.set_xlabel('Nombre de clusters')
            ax2.set_ylabel('Score de Silhouette')
            st.pyplot(fig2)
        
        st.success(f"Le nombre optimal de clusters est : {best_n} (score de silhouette: {silhouette_scores[best_n-2]:.3f})")
        st.session_state.best_n = best_n
        st.session_state.scaled_data = scaled_data
        st.session_state.df_rfm = df_rfm
        st.session_state.selected_cols = selected_cols
        st.session_state.scaler = scaler
        st.session_state.df_rfm_clean = df_rfm_clean
    
    # Application de K-means
    if 'best_n' in st.session_state:
        st.subheader("Application de K-means")
        n_clusters = st.slider(
            "Nombre de clusters", 
            2, max_clusters, st.session_state.best_n,
            key="n_clusters"
        )
        
        if st.button("Ex√©cuter K-means"):
            with st.spinner("Clustering en cours..."):
                kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)
                clusters = kmeans.fit_predict(st.session_state.scaled_data)
                silhouette_avg = silhouette_score(st.session_state.scaled_data, clusters)
                
                # Ajout des clusters aux donn√©es
                df_clustered = st.session_state.df_rfm_clean.copy()
                df_clustered['Cluster'] = clusters
            
            st.success(f"Clustering termin√©! Score de silhouette: {silhouette_avg:.3f}")
            
            # Visualisation des clusters
            st.subheader("Visualisation des Clusters")
            
            if len(st.session_state.selected_cols) >= 2:
                fig = px.scatter(
                    df_clustered, 
                    x=st.session_state.selected_cols[0], 
                    y=st.session_state.selected_cols[1], 
                    color='Cluster',
                    title="Visualisation des Clusters",
                    hover_data=df_clustered.columns
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Profil des clusters
            st.subheader("Profil des Clusters")
            cluster_profile = df_clustered.groupby('Cluster').agg({
                'Recence': ['mean', 'min', 'max'],
                'Frequence': ['mean', 'min', 'max'],
                'Montant': ['mean', 'min', 'max'],
                'CustomerID': 'count'
            }).reset_index()
            
            safe_display_dataframe(cluster_profile)
            
            # Interpr√©tation
            st.subheader("Interpr√©tation des Clusters")
            for cluster in sorted(df_clustered['Cluster'].unique()):
                cluster_data = df_clustered[df_clustered['Cluster'] == cluster]
                recence_moy = cluster_data['Recence'].mean()
                frequence_moy = cluster_data['Frequence'].mean()
                montant_moy = cluster_data['Montant'].mean()
                
                st.markdown(f"**Cluster {cluster}** (Nombre: {len(cluster_data)})")
                st.markdown(f"- R√©cence moyenne: {recence_moy:.1f} jours")
                st.markdown(f"- Fr√©quence moyenne: {frequence_moy:.1f} commandes")
                st.markdown(f"- Montant moyen: {montant_moy:.1f} ‚Ç¨")
                st.markdown("---")
            
            # ===========================================
            # PARTIE CONTRAT DE MAINTENANCE (STABILIT√â TEMPORELLE)
            # ===========================================
            st.subheader("Contrat de Maintenance: √âtude de Stabilit√© Temporelle")
            
            # V√©rification des donn√©es temporelles
            if 'InvoiceDate' not in df.columns:
                st.warning("La colonne 'InvoiceDate' est n√©cessaire pour l'√©tude de stabilit√© temporelle.")
                return
            
            st.markdown("""
            ### Objectif
            √âvaluer la stabilit√© du mod√®le de clustering au fil du temps en comparant les r√©sultats 
            sur diff√©rentes p√©riodes temporelles √† l'aide de l'Adjusted Rand Index (ARI).
            """)
            
            # Fonctions pour l'√©tude de stabilit√©
            def echantillonnage(df, n, pas=30, duree=180):
                date_min = df['InvoiceDate'].min()
                date_min = date_min + timedelta(days=pas * (n-1))
                date_max = date_min + timedelta(days=duree)
                
                sample_df = df[(df['InvoiceDate'] >= date_min) & 
                            (df['InvoiceDate'] < date_max)].copy()
                
                st.info(f"√âchantillon {n}: {sample_df.shape[0]} lignes ({date_min.strftime('%Y-%m-%d')} au {date_max.strftime('%Y-%m-%d')})")
                return sample_df

            def prepare_data(df):
                if 'Montant' not in df.columns:
                    if 'Quantity' in df.columns and 'UnitPrice' in df.columns:
                        df['Montant'] = df['Quantity'] * df['UnitPrice']
                    else:
                        st.error("Impossible de calculer le montant")
                        return None
                
                derniere_date = df['InvoiceDate'].max() + timedelta(days=1)
                
                rfm = df.groupby('CustomerID').agg({
                    'InvoiceDate': lambda x: (derniere_date - x.max()).days,
                    'InvoiceNo': 'nunique',
                    'Montant': 'sum'
                }).rename(columns={
                    'InvoiceDate': 'Recence',
                    'InvoiceNo': 'Frequence',
                    'Montant': 'MontantTotal'
                })
                return rfm

            if st.button("Lancer l'√©tude de stabilit√© temporelle"):
                with st.spinner("Cr√©ation des √©chantillons temporels..."):
                    # Cr√©ation des √©chantillons
                    samples = {}
                    for n in range(1, 10):
                        sample_df = echantillonnage(df, n)
                        sample_rfm = prepare_data(sample_df)
                        if sample_rfm is not None:
                            samples[f"B{n-1}"] = sample_rfm
                    
                    if not samples:
                        st.error("Aucun √©chantillon valide cr√©√©")
                        return
                    
                    st.session_state.samples = samples
                    st.success(f"{len(samples)} √©chantillons temporels cr√©√©s avec succ√®s!")
                
                # Pr√©paration du mod√®le de r√©f√©rence
                with st.spinner("Cr√©ation du mod√®le de r√©f√©rence..."):
                    scaler_ref = StandardScaler()
                    B0_scaled = scaler_ref.fit_transform(samples["B0"])
                    model0 = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)
                    model0.fit(B0_scaled)
                    st.session_state.model0 = model0
                    st.session_state.scaler_ref = scaler_ref
                
                # Fonction d'√©valuation
                def entrainer_modele(data, modele_reference, scaler_ref):
                    data_scaled = scaler_ref.transform(data)
                    nouveau_modele = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)
                    nouveau_modele.fit(data_scaled)
                    labels_ref = modele_reference.predict(scaler_ref.transform(data))
                    labels_nouv = nouveau_modele.labels_
                    ari = adjusted_rand_score(labels_ref, labels_nouv)
                    return ari
                
                # Calcul des scores ARI
                with st.spinner("Calcul des scores de stabilit√©..."):
                    ARI_scores = []
                    sample_names = sorted(st.session_state.samples.keys())
                    
                    for i, name in enumerate(sample_names[1:], start=1):  # Commencer √† B1
                        ari = entrainer_modele(
                            st.session_state.samples[name], 
                            st.session_state.model0, 
                            st.session_state.scaler_ref
                        )
                        ARI_scores.append(ari)
                        st.write(f"√âchantillon B{i}: ARI = {ari:.4f}")
                
                st.session_state.ARI_scores = ARI_scores
            
            # Affichage des r√©sultats de stabilit√©
            if 'ARI_scores' in st.session_state:
                st.subheader("R√©sultats de l'√©tude de stabilit√©")
                
                # Tableau des scores
                results_df = pd.DataFrame({
                    'P√©riode': [f"B{i}" for i in range(1, len(st.session_state.ARI_scores)+1)],
                    'Score ARI': st.session_state.ARI_scores
                })
                safe_display_dataframe(results_df)
                
                # Graphique d'√©volution
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.plot(range(1, len(st.session_state.ARI_scores)+1), st.session_state.ARI_scores, 'o-', markersize=8)
                ax.axhline(y=0.5, color='r', linestyle='--', label='Seuil de stabilit√©')
                ax.set_xlabel('P√©riode')
                ax.set_ylabel('Score ARI')
                ax.set_title('√âvolution de la Stabilit√© du Mod√®le')
                ax.set_xticks(range(1, len(st.session_state.ARI_scores)+1))
                ax.set_xticklabels([f"P√©riode {i}" for i in range(1, len(st.session_state.ARI_scores)+1)])
                ax.legend()
                ax.grid(True)
                st.pyplot(fig)
                
                # Interpr√©tation et recommandations
                st.subheader("Interpr√©tation et Plan de Maintenance")
                
                # Analyse de la stabilit√©
                if st.session_state.ARI_scores:
                    min_ari = min(st.session_state.ARI_scores)
                    if min_ari > 0.7:
                        stability = "excellente"
                        recommendation = "Surveillance trimestrielle suffisante"
                        color = "green"
                    elif min_ari > 0.5:
                        stability = "bonne"
                        recommendation = "Surveillance mensuelle recommand√©e"
                        color = "orange"
                    elif min_ari > 0.3:
                        stability = "mod√©r√©e"
                        recommendation = "R√©√©valuation bimestrielle n√©cessaire"
                        color = "orange"
                    else:
                        stability = "faible"
                        recommendation = "R√©entra√Ænement imm√©diat du mod√®le requis"
                        color = "red"
                    
                    st.markdown(f"""
                    <div style="border-left: 4px solid {color}; padding: 0.5em 1em; background-color: #f8f9fa;">
                        <h4>√âvaluation de la stabilit√©: <span style="color:{color};">{stability}</span></h4>
                        <p>Score ARI minimum: {min_ari:.4f}</p>
                        <p><strong>Recommandation:</strong> {recommendation}</p>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.warning("Aucun score ARI disponible")
                
                # Plan de maintenance
                st.markdown("""
                ### Plan de Maintenance Propos√©
                
                | Activit√©                         | Fr√©quence       | Responsable       |
                |----------------------------------|-----------------|-------------------|
                | Surveillance du score ARI        | Hebdomadaire    | Data Scientist    |
                | Analyse de la stabilit√©          | Mensuelle       | Data Scientist    |
                | R√©√©valuation du mod√®le           | Trimestrielle   | √âquipe Analytics  |
                | R√©entra√Ænement du mod√®le         | Selon besoin    | √âquipe Analytics  |
                | Rapport de performance           | Trimestriel     | Responsable Projet|
                
                **Seuils d'alerte:**
                - Avertissement: Score ARI < 0.5
                - Action imm√©diate: Score ARI < 0.3
                
                **Actions correctives:**
                1. R√©entra√Æner le mod√®le avec les donn√©es r√©centes
                2. R√©√©valuer le nombre optimal de clusters
                3. Investiguer les changements dans le comportement des clients
                4. Mettre √† jour les strat√©gies marketing en fonction des nouveaux segments
                """)

def perform_rfm_analysis(df):
    """Effectue l'analyse RFM"""
    # V√©rification des colonnes n√©cessaires
    required_cols = ['CustomerID', 'InvoiceDate', 'InvoiceNo', 'Montant']
    if 'Montant' not in df.columns:
        if 'Quantity' in df.columns and 'UnitPrice' in df.columns:
            df['Montant'] = df['Quantity'] * df['UnitPrice']
        else:
            st.error("Impossible de calculer le montant: colonnes manquantes")
            return
    
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        st.error(f"Colonnes manquantes pour l'analyse RFM: {', '.join(missing_cols)}")
        return
    
    # Convertir InvoiceDate en datetime si n√©cessaire
    if 'InvoiceDate' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['InvoiceDate']):
        try:
            df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
        except:
            st.error("Impossible de convertir les dates")
            return
    
    # Calcul RFM
    date_ref = df['InvoiceDate'].max() + timedelta(days=1)
    
    rfm = df.groupby('CustomerID').agg({
        'InvoiceDate': lambda x: (date_ref - x.max()).days,
        'InvoiceNo': 'nunique',
        'Montant': 'sum'
    }).reset_index()
    
    rfm.columns = ['CustomerID', 'Recence', 'Frequence', 'Montant']
    
    # Calcul des quartiles
    quantiles = rfm[['Recence', 'Frequence', 'Montant']].quantile([0.25, 0.5, 0.75])
    quantiles = quantiles.to_dict()
    
    # Fonctions de scoring
    def r_score(x):
        if x <= quantiles['Recence'][0.25]:
            return 4
        elif x <= quantiles['Recence'][0.5]:
            return 3
        elif x <= quantiles['Recence'][0.75]:
            return 2
        else:
            return 1
    
    def fm_score(x, var):
        if x <= quantiles[var][0.25]:
            return 1
        elif x <= quantiles[var][0.5]:
            return 2
        elif x <= quantiles[var][0.75]:
            return 3
        else:
            return 4
    
    # Application des scores
    rfm['R'] = rfm['Recence'].apply(r_score)
    rfm['F'] = rfm['Frequence'].apply(lambda x: fm_score(x, 'Frequence'))
    rfm['M'] = rfm['Montant'].apply(lambda x: fm_score(x, 'Montant'))
    rfm['RFM_Score'] = rfm['R'].map(str) + rfm['F'].map(str) + rfm['M'].map(str)
    
    # Segmentation
    contrat = {
        r'[1-2][1-2]': 'en hibernation',
        r'[1-2][3-4]': '√† risque',
        r'[1-2]5': '√† ne pas perdre',
        r'3[1-2]': 'sur le point de dormir',
        r'33': 'n√©cessite de l\'attention',
        r'[3-4][4-5]': 'clients fid√®les',
        r'41': 'prometteurs',
        r'51': 'nouveaux clients',
        r'[4-5][2-3]': 'clients potentiellement fid√®les',
        r'5[4-5]': 'champions'
    }
    
    rfm['Segment'] = rfm['R'].map(str) + rfm['F'].map(str)
    rfm['Segment'] = rfm['Segment'].replace(contrat, regex=True)
    
    # Affichage des r√©sultats
    st.subheader("R√©sultats de l'analyse RFM")
    safe_display_dataframe(rfm.head(10))
    
    # Visualisation
    st.subheader("Visualisation des Segments RFM")
    
    # Distribution des segments
    segment_counts = rfm['Segment'].value_counts().reset_index()
    segment_counts.columns = ['Segment', 'Nombre de Clients']
    
    col1, col2 = st.columns(2)
    with col1:
        fig1 = px.bar(
            segment_counts,
            x='Segment',
            y='Nombre de Clients',
            title="R√©partition des Segments RFM"
        )
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        fig2 = px.pie(
            segment_counts,
            names='Segment',
            values='Nombre de Clients',
            title="Distribution des Segments RFM"
        )
        st.plotly_chart(fig2, use_container_width=True)
    
    # Treemap
    st.subheader("Treemap des Segments RFM")
    plt.figure(figsize=(12, 8))
    colors = ['#f94144', '#f3722c', '#f9c74f', '#90be6d', '#43aa8b',
              '#577590', '#277da1', '#4d908e', '#f9844a', '#8ac926']
    
    squarify.plot(
        sizes=segment_counts['Nombre de Clients'],
        label=segment_counts['Segment'],
        color=colors,
        alpha=.9,
        text_kwargs={'fontsize': 12, 'weight': 'bold'}
    )
    
    plt.title("Segmentation RFM des Clients", fontsize=16)
    plt.axis('off')
    st.pyplot(plt)
    
    # Recommandations
    st.subheader("Recommandations par Segment")
    recommendations = {
        'champions': "R√©compenser, offres exclusives, programmes VIP",
        'clients fid√®les': "Fid√©lisation, offres personnalis√©es",
        'clients potentiellement fid√®les': "Encourager √† acheter plus",
        '√† ne pas perdre': "Offres sp√©ciales, attention particuli√®re",
        'prometteurs': "Encourager √† devenir fid√®les",
        '√† risque': "Campagnes de r√©activation",
        'sur le point de dormir': "Relances par email, offres de retour",
        'n√©cessite de l\'attention': "Offres cibl√©es, rappels",
        'nouveaux clients': "Bienvenue, guide d'utilisation",
        'en hibernation': "Campagnes agressives de r√©activation"
    }
    
    rec_df = pd.DataFrame.from_dict(recommendations, orient='index', columns=['Recommandation'])
    safe_display_dataframe(rec_df)

# Interface principale
def main():
    # Chargement de l'image d'accueil
    image_url = "https://images.pexels.com/photos/669615/pexels-photo-669615.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1"
    
    # Guide d'utilisation dans la barre lat√©rale
    st.sidebar.header("üìö Guide d'Utilisation")
    with st.sidebar.expander("Comment utiliser cette application"):
        st.markdown("""
        ### √âtapes d'utilisation :
        1. **Charger les donn√©es** : Utilisez le s√©lecteur de fichiers pour uploader vos donn√©es transactionnelles
        2. **Nettoyer les donn√©es** : Cliquez sur 'Nettoyer les donn√©es' dans l'onglet Statistiques
        3. **Explorer les donn√©es** : Consultez les statistiques descriptives apr√®s nettoyage
        4. **Analyser les donn√©es** :
           - **FP-Growth** : D√©couvrir les associations entre produits
           - **K-means** : Segmenter les clients avec analyse de stabilit√©
           - **RFM** : Segmenter clients par R√©cence, Fr√©quence, Montant
        
        ### Format de donn√©es recommand√© :
        Les donn√©es doivent contenir au minimum ces colonnes :
        - `CustomerID` : Identifiant unique client
        - `InvoiceNo` : Num√©ro de facture
        - `InvoiceDate` : Date de la transaction
        - `Description` : Description du produit
        - `Quantity` : Quantit√© achet√©e
        - `UnitPrice` : Prix unitaire
        
        ### Conseils :
        - Les fichiers CSV sont pr√©f√©rables aux Excel
        - La taille maximale recommand√©e est de 200 Mo
        -
        
        ### Fonctionnalit√©s avanc√©es :
        - Contrat de maintenance pour le clustering K-means
        - Export des r√©sultats au format CSV
        - Visualisations interactives avec Plotly
        """)
    
    st.title("üõí Plateforme d'Analyse e-commerce")
    
    # Chargement des donn√©es
    st.sidebar.header("Chargement des Donn√©es")
    uploaded_file = st.sidebar.file_uploader("Uploader votre fichier de donn√©es (CSV ou Excel)", type=['csv', 'xlsx'])
    
    df = None
    df_clean = None
    
    if uploaded_file is not None:
        df = load_data(uploaded_file)
        
        if df is not None:
            # Onglets principaux
            tab_stats, tab_fp, tab_kmeans, tab_rfm = st.tabs([
                "Statistiques", 
                "FP-Growth", 
                "K-means", 
                "RFM"
            ])
            
            with tab_stats:
                # Statistiques globales
                st.header("Statistiques Globales")
                stats_df = global_stats(df)
                safe_display_dataframe(stats_df)
                
                # Nettoyage des donn√©es
                if st.button("Nettoyer les donn√©es", key="clean_data_btn"):
                    with st.spinner("Nettoyage en cours..."):
                        df_clean = clean_data(df)
                    st.success("Donn√©es nettoy√©es avec succ√®s!")
                    st.session_state.df_clean = df_clean
                
                # Afficher les stats descriptives si donn√©es nettoy√©es
                if 'df_clean' in st.session_state:
                    st.header("Donn√©es Apr√®s Nettoyage")
                    st.write(f"Dimensions: {st.session_state.df_clean.shape[0]} lignes, {st.session_state.df_clean.shape[1]} colonnes")
                    show_descriptive_stats(st.session_state.df_clean)
                else:
                    st.info("Cliquez sur 'Nettoyer les donn√©es' pour commencer l'analyse")
            
            # Onglet FP-Growth
            with tab_fp:
                st.header("Analyse par R√®gles d'Association (FP-Growth)")
                if 'df_clean' in st.session_state:
                    perform_fpgrowth_analysis(st.session_state.df_clean)
                else:
                    st.info("Veuillez d'abord nettoyer les donn√©es dans l'onglet Statistiques")
            
            # Onglet K-means
            with tab_kmeans:
                st.header("Segmentation des Clients (K-means)")
                if 'df_clean' in st.session_state:
                    perform_kmeans_analysis(st.session_state.df_clean)
                else:
                    st.info("Veuillez d'abord nettoyer les donn√©es dans l'onglet Statistiques")
            
            # Onglet RFM
            with tab_rfm:
                st.header("Analyse RFM (R√©cence, Fr√©quence, Montant)")
                if 'df_clean' in st.session_state:
                    perform_rfm_analysis(st.session_state.df_clean)
                else:
                    st.info("Veuillez d'abord nettoyer les donn√©es dans l'onglet Statistiques")
    else:
        # Page d'accueil avec image
        st.markdown("""
        <style>
        .big-font {
            font-size:20px !important;
        }
        </style>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        ## Bienvenue dans la plateforme d'analyse e-commerce !
        Cette application permet d'analyser les donn√©es clients d'un site e-commerce √† l'aide de trois approches:
        - **R√®gles d'association (FP-Growth)**: D√©couvrir quels produits sont fr√©quemment achet√©s ensemble
        - **Segmentation (K-means)**: Grouper les clients en clusters similaires
        - **Analyse RFM**: Segmenter les clients bas√© sur la R√©cence, Fr√©quence et Montant des achats
        """)
        
        # Chargement et affichage de l'image
        try:
            response = requests.get(image_url)
            img = Image.open(BytesIO(response.content))
            st.image(img, caption="Plateforme d'analyse e-commerce", use_container_width=True)
        except:
            st.warning("Impossible de charger l'image d'accueil")
        
        st.markdown("""
        ### Pour commencer :
        1. Utilisez le panneau lat√©ral gauche pour uploader vos donn√©es
        2. Consultez le guide d'utilisation pour plus d'informations
        3. Explorez les diff√©rentes analyses disponibles
        
        <div class="big-font">
        ‚¨ÖÔ∏è <strong>Commencez par uploader vos donn√©es dans la barre lat√©rale !</strong>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
